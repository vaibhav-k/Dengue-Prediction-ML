{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the proteins data\n",
    "ns2b = []\n",
    "ns2bd = []\n",
    "       \n",
    "f = open(\"../../../../../../Data/Proteins/DENV1/NS2B/DENV1_NS2B.txt\", \"r\")\n",
    "for x in f:\n",
    "    if \"DSS\" in x:\n",
    "        ns2bd.append(1)\n",
    "    elif \"DHF\" in x:\n",
    "        ns2bd.append(1)\n",
    "    elif x[0] == \">\":\n",
    "        ns2bd.append(0)\n",
    "    else:\n",
    "        x = x.replace(\"\\n\", \"\")\n",
    "        ns2b.append(x)\n",
    "        \n",
    "# Converting the array into DataFrame\n",
    "ns2b = pd.DataFrame(ns2b)\n",
    "\n",
    "# Attaching the \"Disease\" label column to the input\n",
    "ns2b[\"Disease\"] = ns2bd\n",
    "\n",
    "# Renaming the columns\n",
    "ns2b = ns2b.rename(index=str, columns={0: \"Sequence\", \"Disease\": \"Disease\"})\n",
    "\n",
    "# clearing the memory\n",
    "del ns2bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined shape of the given data is: (999, 2)\n",
      "The length of the combined data is: 999\n",
      "Does the combined data have any null value? -> False\n"
     ]
    }
   ],
   "source": [
    "print(\"The combined shape of the given data is:\", str(ns2b.shape))\n",
    "print(\"The length of the combined data is:\", str(len(ns2b.index)))\n",
    "print(\"Does the combined data have any null value? ->\", ns2b.isnull().values.any())\n",
    "\n",
    "ns2b = ns2b.dropna(how = 'any',axis = 0) \n",
    "\n",
    "# Shuffling the data and then taking a peek\n",
    "ns2b = ns2b.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sequence strings into k-mer words, default size = 6 (hexamer words)\n",
    "def getKmers(sequence, size = 6):\n",
    "    return [sequence[x:x + size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "ns2b['words'] = ns2b.apply(lambda x: getKmers(x['Sequence']), axis=1)\n",
    "ns2b = ns2b.drop('Sequence', axis=1)\n",
    "\n",
    "ns2b_texts = list(ns2b['words'])\n",
    "for item in range(len(ns2b_texts)):\n",
    "    ns2b_texts[item] = ' '.join(ns2b_texts[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of y is: (999,)\n"
     ]
    }
   ],
   "source": [
    "# Creating y and printing the shape of it\n",
    "y = ns2b.iloc[:, 0].values\n",
    "print(\"The shape of y is:\", y.shape)\n",
    "\n",
    "# clearing the memory\n",
    "del ns2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model using CountVectorizer()\n",
    "# This is equivalent to k-mer counting\n",
    "# The n-gram size of 4 was previously determined by testing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(4,4))\n",
    "x = cv.fit_transform(ns2b_texts)\n",
    "\n",
    "# clearing the memory\n",
    "del cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train is: (799, 643)\n",
      "The shape of y_train is: (799,)\n",
      "The shape of x_test is: (200, 643)\n",
      "The shape of y_test is: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the human dataset into the training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42, stratify=y)\n",
    "\n",
    "# Printing the shapes of the train and test matrices\n",
    "print(\"The shape of x_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of x_test is:\", X_test.shape)\n",
    "print(\"The shape of y_test is:\", y_test.shape)\n",
    "\n",
    "# clearing the memory\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0323 15:36:00.561651 139657508943680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0323 15:36:00.580739 139657508943680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0323 15:36:00.582691 139657508943680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=643, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 15:36:00.633797 139657508943680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0323 15:36:00.655977 139657508943680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0323 15:36:00.661093 139657508943680 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0323 15:36:00.837540 139657508943680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "799/799 [==============================] - 0s 510us/step - loss: 0.6546 - acc: 0.5807\n",
      "Epoch 2/150\n",
      "799/799 [==============================] - 0s 310us/step - loss: 0.6141 - acc: 0.5732\n",
      "Epoch 3/150\n",
      "799/799 [==============================] - 0s 257us/step - loss: 0.6105 - acc: 0.5820\n",
      "Epoch 4/150\n",
      "799/799 [==============================] - 0s 391us/step - loss: 0.6070 - acc: 0.5970\n",
      "Epoch 5/150\n",
      "799/799 [==============================] - 0s 323us/step - loss: 0.6062 - acc: 0.5982\n",
      "Epoch 6/150\n",
      "799/799 [==============================] - 0s 317us/step - loss: 0.6020 - acc: 0.6045\n",
      "Epoch 7/150\n",
      "799/799 [==============================] - 0s 424us/step - loss: 0.6003 - acc: 0.5945\n",
      "Epoch 8/150\n",
      "799/799 [==============================] - 0s 327us/step - loss: 0.6017 - acc: 0.5995\n",
      "Epoch 9/150\n",
      "799/799 [==============================] - 0s 294us/step - loss: 0.5974 - acc: 0.5832\n",
      "Epoch 10/150\n",
      "799/799 [==============================] - 0s 351us/step - loss: 0.6002 - acc: 0.5957\n",
      "Epoch 11/150\n",
      "799/799 [==============================] - 0s 325us/step - loss: 0.5958 - acc: 0.5945\n",
      "Epoch 12/150\n",
      "799/799 [==============================] - 0s 308us/step - loss: 0.5959 - acc: 0.5807\n",
      "Epoch 13/150\n",
      "799/799 [==============================] - 0s 316us/step - loss: 0.5905 - acc: 0.5882\n",
      "Epoch 14/150\n",
      "799/799 [==============================] - 0s 379us/step - loss: 0.5876 - acc: 0.5995\n",
      "Epoch 15/150\n",
      "799/799 [==============================] - 0s 293us/step - loss: 0.5896 - acc: 0.5970\n",
      "Epoch 16/150\n",
      "799/799 [==============================] - 0s 354us/step - loss: 0.5857 - acc: 0.5920\n",
      "Epoch 17/150\n",
      "799/799 [==============================] - 0s 298us/step - loss: 0.5850 - acc: 0.5832\n",
      "Epoch 18/150\n",
      "799/799 [==============================] - 0s 295us/step - loss: 0.5830 - acc: 0.6108\n",
      "Epoch 19/150\n",
      "799/799 [==============================] - 0s 280us/step - loss: 0.5869 - acc: 0.5957\n",
      "Epoch 20/150\n",
      "799/799 [==============================] - 0s 354us/step - loss: 0.5807 - acc: 0.6008\n",
      "Epoch 21/150\n",
      "799/799 [==============================] - 0s 322us/step - loss: 0.5947 - acc: 0.6158\n",
      "Epoch 22/150\n",
      "799/799 [==============================] - 0s 307us/step - loss: 0.5870 - acc: 0.5982\n",
      "Epoch 23/150\n",
      "799/799 [==============================] - 0s 319us/step - loss: 0.5842 - acc: 0.5970\n",
      "Epoch 24/150\n",
      "799/799 [==============================] - 0s 245us/step - loss: 0.5844 - acc: 0.6045\n",
      "Epoch 25/150\n",
      "799/799 [==============================] - 0s 284us/step - loss: 0.5806 - acc: 0.6070\n",
      "Epoch 26/150\n",
      "799/799 [==============================] - 0s 253us/step - loss: 0.5820 - acc: 0.6008\n",
      "Epoch 27/150\n",
      "799/799 [==============================] - 0s 250us/step - loss: 0.5826 - acc: 0.6095\n",
      "Epoch 28/150\n",
      "799/799 [==============================] - 0s 257us/step - loss: 0.5787 - acc: 0.6070\n",
      "Epoch 29/150\n",
      "799/799 [==============================] - 0s 253us/step - loss: 0.5789 - acc: 0.6020\n",
      "Epoch 30/150\n",
      "799/799 [==============================] - 0s 238us/step - loss: 0.5800 - acc: 0.5695\n",
      "Epoch 31/150\n",
      "799/799 [==============================] - 0s 309us/step - loss: 0.5775 - acc: 0.5982\n",
      "Epoch 32/150\n",
      "799/799 [==============================] - 0s 260us/step - loss: 0.5784 - acc: 0.5895\n",
      "Epoch 33/150\n",
      "799/799 [==============================] - 0s 265us/step - loss: 0.5783 - acc: 0.5995\n",
      "Epoch 34/150\n",
      "799/799 [==============================] - 0s 314us/step - loss: 0.5778 - acc: 0.5945\n",
      "Epoch 35/150\n",
      "799/799 [==============================] - 0s 246us/step - loss: 0.5774 - acc: 0.6145\n",
      "Epoch 36/150\n",
      "799/799 [==============================] - 0s 263us/step - loss: 0.5770 - acc: 0.5845\n",
      "Epoch 37/150\n",
      "799/799 [==============================] - 0s 231us/step - loss: 0.5790 - acc: 0.5945\n",
      "Epoch 38/150\n",
      "799/799 [==============================] - 0s 261us/step - loss: 0.5760 - acc: 0.6108\n",
      "Epoch 39/150\n",
      "799/799 [==============================] - 0s 256us/step - loss: 0.5766 - acc: 0.6133\n",
      "Epoch 40/150\n",
      "799/799 [==============================] - 0s 158us/step - loss: 0.5773 - acc: 0.5945\n",
      "Epoch 41/150\n",
      "799/799 [==============================] - 0s 118us/step - loss: 0.5754 - acc: 0.5857\n",
      "Epoch 42/150\n",
      "799/799 [==============================] - 0s 160us/step - loss: 0.5753 - acc: 0.6108\n",
      "Epoch 43/150\n",
      "799/799 [==============================] - 0s 276us/step - loss: 0.5784 - acc: 0.6058\n",
      "Epoch 44/150\n",
      "799/799 [==============================] - 0s 298us/step - loss: 0.5760 - acc: 0.6020\n",
      "Epoch 45/150\n",
      "799/799 [==============================] - 0s 286us/step - loss: 0.5769 - acc: 0.6070\n",
      "Epoch 46/150\n",
      "799/799 [==============================] - 0s 244us/step - loss: 0.5778 - acc: 0.6170\n",
      "Epoch 47/150\n",
      "799/799 [==============================] - 0s 285us/step - loss: 0.5783 - acc: 0.5932\n",
      "Epoch 48/150\n",
      "799/799 [==============================] - 0s 296us/step - loss: 0.5757 - acc: 0.6108\n",
      "Epoch 49/150\n",
      "799/799 [==============================] - 0s 263us/step - loss: 0.5763 - acc: 0.6045\n",
      "Epoch 50/150\n",
      "799/799 [==============================] - 0s 306us/step - loss: 0.5778 - acc: 0.5707\n",
      "Epoch 51/150\n",
      "799/799 [==============================] - 0s 313us/step - loss: 0.5782 - acc: 0.5957\n",
      "Epoch 52/150\n",
      "799/799 [==============================] - 0s 313us/step - loss: 0.5757 - acc: 0.6208\n",
      "Epoch 53/150\n",
      "799/799 [==============================] - 0s 305us/step - loss: 0.5765 - acc: 0.5995\n",
      "Epoch 54/150\n",
      "799/799 [==============================] - 0s 378us/step - loss: 0.5743 - acc: 0.6133\n",
      "Epoch 55/150\n",
      "799/799 [==============================] - 0s 299us/step - loss: 0.5740 - acc: 0.6220\n",
      "Epoch 56/150\n",
      "799/799 [==============================] - 0s 311us/step - loss: 0.5769 - acc: 0.5845\n",
      "Epoch 57/150\n",
      "799/799 [==============================] - 0s 320us/step - loss: 0.5750 - acc: 0.6033\n",
      "Epoch 58/150\n",
      "799/799 [==============================] - 0s 319us/step - loss: 0.5745 - acc: 0.5970\n",
      "Epoch 59/150\n",
      "799/799 [==============================] - 0s 305us/step - loss: 0.5744 - acc: 0.5957\n",
      "Epoch 60/150\n",
      "799/799 [==============================] - 0s 274us/step - loss: 0.5745 - acc: 0.6120\n",
      "Epoch 61/150\n",
      "799/799 [==============================] - 0s 286us/step - loss: 0.5740 - acc: 0.5957\n",
      "Epoch 62/150\n",
      "799/799 [==============================] - 0s 284us/step - loss: 0.5737 - acc: 0.5882\n",
      "Epoch 63/150\n",
      "799/799 [==============================] - 0s 270us/step - loss: 0.5726 - acc: 0.6120\n",
      "Epoch 64/150\n",
      "799/799 [==============================] - 0s 244us/step - loss: 0.5732 - acc: 0.6133\n",
      "Epoch 65/150\n",
      "799/799 [==============================] - 0s 289us/step - loss: 0.5727 - acc: 0.5945\n",
      "Epoch 66/150\n",
      "799/799 [==============================] - 0s 237us/step - loss: 0.5724 - acc: 0.6108\n",
      "Epoch 67/150\n",
      "799/799 [==============================] - 0s 250us/step - loss: 0.5730 - acc: 0.6120\n",
      "Epoch 68/150\n",
      "799/799 [==============================] - 0s 337us/step - loss: 0.5751 - acc: 0.5932\n",
      "Epoch 69/150\n",
      "799/799 [==============================] - 0s 293us/step - loss: 0.5715 - acc: 0.5845\n",
      "Epoch 70/150\n",
      "799/799 [==============================] - 0s 292us/step - loss: 0.5753 - acc: 0.6020\n",
      "Epoch 71/150\n",
      "799/799 [==============================] - 0s 280us/step - loss: 0.5740 - acc: 0.5832\n",
      "Epoch 72/150\n",
      "799/799 [==============================] - 0s 258us/step - loss: 0.5726 - acc: 0.6008\n",
      "Epoch 73/150\n",
      "799/799 [==============================] - 0s 307us/step - loss: 0.5721 - acc: 0.6170\n",
      "Epoch 74/150\n",
      "799/799 [==============================] - 0s 280us/step - loss: 0.5716 - acc: 0.6095\n",
      "Epoch 75/150\n",
      "799/799 [==============================] - 0s 297us/step - loss: 0.5732 - acc: 0.6045\n",
      "Epoch 76/150\n",
      "799/799 [==============================] - 0s 256us/step - loss: 0.5723 - acc: 0.6095\n",
      "Epoch 77/150\n",
      "799/799 [==============================] - 0s 268us/step - loss: 0.5741 - acc: 0.5982\n",
      "Epoch 78/150\n",
      "799/799 [==============================] - 0s 247us/step - loss: 0.5719 - acc: 0.5907\n",
      "Epoch 79/150\n",
      "799/799 [==============================] - 0s 243us/step - loss: 0.5720 - acc: 0.6108\n",
      "Epoch 80/150\n",
      "799/799 [==============================] - 0s 259us/step - loss: 0.5739 - acc: 0.6120\n",
      "Epoch 81/150\n",
      "799/799 [==============================] - 0s 231us/step - loss: 0.5721 - acc: 0.5870\n",
      "Epoch 82/150\n",
      "799/799 [==============================] - 0s 227us/step - loss: 0.5718 - acc: 0.6133\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799/799 [==============================] - 0s 242us/step - loss: 0.5724 - acc: 0.6008\n",
      "Epoch 84/150\n",
      "799/799 [==============================] - 0s 232us/step - loss: 0.5732 - acc: 0.5832\n",
      "Epoch 85/150\n",
      "799/799 [==============================] - 0s 218us/step - loss: 0.5730 - acc: 0.6108\n",
      "Epoch 86/150\n",
      "799/799 [==============================] - 0s 238us/step - loss: 0.5721 - acc: 0.5907\n",
      "Epoch 87/150\n",
      "799/799 [==============================] - 0s 226us/step - loss: 0.5713 - acc: 0.6133\n",
      "Epoch 88/150\n",
      "799/799 [==============================] - 0s 235us/step - loss: 0.5795 - acc: 0.5832\n",
      "Epoch 89/150\n",
      "799/799 [==============================] - 0s 226us/step - loss: 0.5751 - acc: 0.6083\n",
      "Epoch 90/150\n",
      "799/799 [==============================] - 0s 215us/step - loss: 0.5734 - acc: 0.6158\n",
      "Epoch 91/150\n",
      "799/799 [==============================] - 0s 223us/step - loss: 0.5747 - acc: 0.6170\n",
      "Epoch 92/150\n",
      "799/799 [==============================] - 0s 220us/step - loss: 0.5725 - acc: 0.6058\n",
      "Epoch 93/150\n",
      "799/799 [==============================] - 0s 208us/step - loss: 0.5722 - acc: 0.6058\n",
      "Epoch 94/150\n",
      "799/799 [==============================] - 0s 224us/step - loss: 0.5711 - acc: 0.5895\n",
      "Epoch 95/150\n",
      "799/799 [==============================] - 0s 238us/step - loss: 0.5721 - acc: 0.6070\n",
      "Epoch 96/150\n",
      "799/799 [==============================] - 0s 213us/step - loss: 0.5734 - acc: 0.5982\n",
      "Epoch 97/150\n",
      "799/799 [==============================] - 0s 224us/step - loss: 0.5736 - acc: 0.6120\n",
      "Epoch 98/150\n",
      "799/799 [==============================] - 0s 220us/step - loss: 0.5714 - acc: 0.5957\n",
      "Epoch 99/150\n",
      "799/799 [==============================] - 0s 214us/step - loss: 0.5713 - acc: 0.5957\n",
      "Epoch 100/150\n",
      "799/799 [==============================] - 0s 230us/step - loss: 0.5740 - acc: 0.5995\n",
      "Epoch 101/150\n",
      "799/799 [==============================] - 0s 223us/step - loss: 0.5717 - acc: 0.6058\n",
      "Epoch 102/150\n",
      "799/799 [==============================] - 0s 211us/step - loss: 0.5723 - acc: 0.6033\n",
      "Epoch 103/150\n",
      "799/799 [==============================] - 0s 228us/step - loss: 0.5701 - acc: 0.6008\n",
      "Epoch 104/150\n",
      "799/799 [==============================] - 0s 222us/step - loss: 0.5734 - acc: 0.6108\n",
      "Epoch 105/150\n",
      "799/799 [==============================] - 0s 213us/step - loss: 0.5726 - acc: 0.5795\n",
      "Epoch 106/150\n",
      "799/799 [==============================] - 0s 236us/step - loss: 0.5721 - acc: 0.6133\n",
      "Epoch 107/150\n",
      "799/799 [==============================] - 0s 215us/step - loss: 0.5715 - acc: 0.6008\n",
      "Epoch 108/150\n",
      "799/799 [==============================] - 0s 216us/step - loss: 0.5712 - acc: 0.5932\n",
      "Epoch 109/150\n",
      "799/799 [==============================] - 0s 227us/step - loss: 0.5711 - acc: 0.5895\n",
      "Epoch 110/150\n",
      "799/799 [==============================] - 0s 227us/step - loss: 0.5719 - acc: 0.5920\n",
      "Epoch 111/150\n",
      "799/799 [==============================] - 0s 223us/step - loss: 0.5702 - acc: 0.6133\n",
      "Epoch 112/150\n",
      "799/799 [==============================] - 0s 257us/step - loss: 0.5721 - acc: 0.5907\n",
      "Epoch 113/150\n",
      "799/799 [==============================] - 0s 230us/step - loss: 0.5705 - acc: 0.6058\n",
      "Epoch 114/150\n",
      "799/799 [==============================] - 0s 234us/step - loss: 0.5709 - acc: 0.6133\n",
      "Epoch 115/150\n",
      "799/799 [==============================] - 0s 284us/step - loss: 0.5705 - acc: 0.6008\n",
      "Epoch 116/150\n",
      "799/799 [==============================] - 0s 271us/step - loss: 0.5701 - acc: 0.6045\n",
      "Epoch 117/150\n",
      "799/799 [==============================] - 0s 278us/step - loss: 0.5709 - acc: 0.5895\n",
      "Epoch 118/150\n",
      "799/799 [==============================] - 0s 276us/step - loss: 0.5725 - acc: 0.6108\n",
      "Epoch 119/150\n",
      "799/799 [==============================] - 0s 247us/step - loss: 0.5714 - acc: 0.5932\n",
      "Epoch 120/150\n",
      "799/799 [==============================] - 0s 276us/step - loss: 0.5714 - acc: 0.5895\n",
      "Epoch 121/150\n",
      "799/799 [==============================] - 0s 266us/step - loss: 0.5711 - acc: 0.6033\n",
      "Epoch 122/150\n",
      "799/799 [==============================] - 0s 237us/step - loss: 0.5708 - acc: 0.5907\n",
      "Epoch 123/150\n",
      "799/799 [==============================] - 0s 296us/step - loss: 0.5704 - acc: 0.6095\n",
      "Epoch 124/150\n",
      "799/799 [==============================] - 0s 253us/step - loss: 0.5708 - acc: 0.6133\n",
      "Epoch 125/150\n",
      "799/799 [==============================] - 0s 289us/step - loss: 0.5707 - acc: 0.6020\n",
      "Epoch 126/150\n",
      "799/799 [==============================] - 0s 285us/step - loss: 0.5707 - acc: 0.6133\n",
      "Epoch 127/150\n",
      "799/799 [==============================] - 0s 339us/step - loss: 0.5733 - acc: 0.5882\n",
      "Epoch 128/150\n",
      "799/799 [==============================] - 0s 288us/step - loss: 0.5709 - acc: 0.6020\n",
      "Epoch 129/150\n",
      "799/799 [==============================] - 0s 332us/step - loss: 0.5712 - acc: 0.6095\n",
      "Epoch 130/150\n",
      "799/799 [==============================] - 0s 253us/step - loss: 0.5707 - acc: 0.6133\n",
      "Epoch 131/150\n",
      "799/799 [==============================] - 0s 326us/step - loss: 0.5711 - acc: 0.6133\n",
      "Epoch 132/150\n",
      "799/799 [==============================] - 0s 337us/step - loss: 0.5705 - acc: 0.5970\n",
      "Epoch 133/150\n",
      "799/799 [==============================] - 0s 362us/step - loss: 0.5708 - acc: 0.6133\n",
      "Epoch 134/150\n",
      "799/799 [==============================] - 0s 319us/step - loss: 0.5697 - acc: 0.5945\n",
      "Epoch 135/150\n",
      "799/799 [==============================] - 0s 399us/step - loss: 0.5715 - acc: 0.6145\n",
      "Epoch 136/150\n",
      "799/799 [==============================] - 0s 242us/step - loss: 0.5710 - acc: 0.5932\n",
      "Epoch 137/150\n",
      "799/799 [==============================] - 0s 338us/step - loss: 0.5702 - acc: 0.6133\n",
      "Epoch 138/150\n",
      "799/799 [==============================] - 0s 292us/step - loss: 0.5717 - acc: 0.6145\n",
      "Epoch 139/150\n",
      "799/799 [==============================] - 0s 276us/step - loss: 0.5701 - acc: 0.5882\n",
      "Epoch 140/150\n",
      "799/799 [==============================] - 0s 332us/step - loss: 0.5712 - acc: 0.5957\n",
      "Epoch 141/150\n",
      "799/799 [==============================] - 0s 330us/step - loss: 0.5710 - acc: 0.6133\n",
      "Epoch 142/150\n",
      "799/799 [==============================] - 0s 411us/step - loss: 0.5707 - acc: 0.5945\n",
      "Epoch 143/150\n",
      "799/799 [==============================] - 0s 464us/step - loss: 0.5698 - acc: 0.6045\n",
      "Epoch 144/150\n",
      "799/799 [==============================] - 0s 401us/step - loss: 0.5716 - acc: 0.6033\n",
      "Epoch 145/150\n",
      "799/799 [==============================] - 0s 379us/step - loss: 0.5725 - acc: 0.6008\n",
      "Epoch 146/150\n",
      "799/799 [==============================] - 0s 374us/step - loss: 0.5701 - acc: 0.6145\n",
      "Epoch 147/150\n",
      "799/799 [==============================] - 0s 332us/step - loss: 0.5710 - acc: 0.6008\n",
      "Epoch 148/150\n",
      "799/799 [==============================] - 0s 328us/step - loss: 0.5707 - acc: 0.6083\n",
      "Epoch 149/150\n",
      "799/799 [==============================] - 0s 339us/step - loss: 0.5712 - acc: 0.6120\n",
      "Epoch 150/150\n",
      "799/799 [==============================] - 0s 263us/step - loss: 0.5714 - acc: 0.5807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f044883fc88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 334us/step\n",
      "Accuracy: 60.50\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "# clearing the memory\n",
    "del accuracy\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
