{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the proteins data\n",
    "ns2b = []\n",
    "ns2bd = []\n",
    "       \n",
    "f = open(\"../../../../../../Data/Proteins/DENV1/NS2B/DENV1_NS2B.txt\", \"r\")\n",
    "for x in f:\n",
    "    if \"DSS\" in x:\n",
    "        ns2bd.append(1)\n",
    "    elif \"DHF\" in x:\n",
    "        ns2bd.append(1)\n",
    "    elif x[0] == \">\":\n",
    "        ns2bd.append(0)\n",
    "    else:\n",
    "        x = x.replace(\"\\n\", \"\")\n",
    "        ns2b.append(x)\n",
    "        \n",
    "# Converting the array into DataFrame\n",
    "ns2b = pd.DataFrame(ns2b)\n",
    "\n",
    "# Attaching the \"Disease\" label column to the input\n",
    "ns2b[\"Disease\"] = ns2bd\n",
    "\n",
    "# Renaming the columns\n",
    "ns2b = ns2b.rename(index=str, columns={0: \"Sequence\", \"Disease\": \"Disease\"})\n",
    "\n",
    "# clearing the memory\n",
    "del ns2bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined shape of the given data is: (999, 2)\n",
      "The length of the combined data is: 999\n",
      "Does the combined data have any null value? -> False\n"
     ]
    }
   ],
   "source": [
    "print(\"The combined shape of the given data is:\", str(ns2b.shape))\n",
    "print(\"The length of the combined data is:\", str(len(ns2b.index)))\n",
    "print(\"Does the combined data have any null value? ->\", ns2b.isnull().values.any())\n",
    "\n",
    "ns2b = ns2b.dropna(how = 'any',axis = 0) \n",
    "\n",
    "# Shuffling the data and then taking a peek\n",
    "ns2b = ns2b.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sequence strings into k-mer words, default size = 6 (hexamer words)\n",
    "def getKmers(sequence, size = 6):\n",
    "    return [sequence[x:x + size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "ns2b['words'] = ns2b.apply(lambda x: getKmers(x['Sequence']), axis=1)\n",
    "ns2b = ns2b.drop('Sequence', axis=1)\n",
    "\n",
    "ns2b_texts = list(ns2b['words'])\n",
    "for item in range(len(ns2b_texts)):\n",
    "    ns2b_texts[item] = ' '.join(ns2b_texts[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of y is: (999,)\n"
     ]
    }
   ],
   "source": [
    "# Creating y and printing the shape of it\n",
    "y = ns2b.iloc[:, 0].values\n",
    "print(\"The shape of y is:\", y.shape)\n",
    "\n",
    "# clearing the memory\n",
    "del ns2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model using CountVectorizer()\n",
    "# This is equivalent to k-mer counting\n",
    "# The n-gram size of 4 was previously determined by testing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range = (4,4))\n",
    "x = cv.fit_transform(ns2b_texts)\n",
    "\n",
    "# clearing the memory\n",
    "del cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train is: (799, 643)\n",
      "The shape of y_train is: (799,)\n",
      "The shape of x_test is: (200, 643)\n",
      "The shape of y_test is: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the human dataset into the training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42, stratify=y)\n",
    "\n",
    "# Printing the shapes of the train and test matrices\n",
    "print(\"The shape of x_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of x_test is:\", X_test.shape)\n",
    "print(\"The shape of y_test is:\", y_test.shape)\n",
    "\n",
    "# clearing the memory\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, C = 0.0001)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# clearing the memory\n",
    "del classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "Predicted    0\n",
      "Actual        \n",
      "0          120\n",
      "1           80\n",
      "\n",
      "accuracy = 0.6 \n",
      "precision = 0.36 \n",
      "recall = 0.6 \n",
      "f1 = 0.44999999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "print(\"Confusion matrix\")\n",
    "print(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\n",
    "\n",
    "def get_metrics(y_test, y_predicted):\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
    "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\n",
    "print(\"\\naccuracy = {} \\nprecision = {} \\nrecall = {} \\nf1 = {}\".format(accuracy, precision, recall, f1))\n",
    "\n",
    "# clearing the memory\n",
    "del y_pred\n",
    "del accuracy\n",
    "del precision\n",
    "del recall\n",
    "del f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
