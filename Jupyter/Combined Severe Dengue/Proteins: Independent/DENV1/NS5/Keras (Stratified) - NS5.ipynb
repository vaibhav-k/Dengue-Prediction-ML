{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the protein data\n",
    "k2 = []\n",
    "k2d = []\n",
    "\n",
    "f = open(\"../../../../../../Data/Proteins/DENV1/NS5/DENV1_NS5.txt\", \"r\")\n",
    "for x in f:\n",
    "    if \"DSS\" in x:\n",
    "        k2d.append(1)\n",
    "    elif \"DHF\" in x:\n",
    "        k2d.append(1)\n",
    "    elif x[0] == \">\":\n",
    "        k2d.append(0)\n",
    "    else:\n",
    "        x = x.replace(\"\\n\", \"\")\n",
    "        k2.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the array into DataFrame\n",
    "k2 = pd.DataFrame(k2)\n",
    "\n",
    "# Attaching the \"Disease\" label column to the input\n",
    "k2[\"Disease\"] = k2d\n",
    "\n",
    "# Renaming the columns\n",
    "k2 = k2.rename(index=str, columns={0: \"Sequence\", \"Disease\": \"Disease\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined shape of the given data is: (999, 2)\n",
      "The length of the combined data is: 999\n",
      "Does the combined data have any null value? -> False\n",
      "                                              Sequence  Disease\n",
      "128  GTGAQGETLGEKWKRQLNQLSKSEFNTYKRSGIMEVDRSEAKEGLK...        0\n",
      "135  GTGAQGETLGEKWKRQLNQLSKSEFNTYKRSGIMEVDRSEAKEGLK...        0\n",
      "831  GTGAQGETLGEKWKRQLNQLSKSEFNTYKRSGIMEVDRSEAKEGLK...        1\n",
      "490  GTGAQGETLGEKWKRQLNQLSKSEFNTYKRSGIMEVDRSEAKEGLK...        0\n",
      "941  GTGAQGETLGEKWKRQLNQLSKSEFNTYKRSGIMEVDRSEAKEGLK...        1\n"
     ]
    }
   ],
   "source": [
    "print(\"The combined shape of the given data is:\", str(k2.shape))\n",
    "print(\"The length of the combined data is:\", str(len(k2.index)))\n",
    "print(\"Does the combined data have any null value? ->\", k2.isnull().values.any())\n",
    "\n",
    "k2 = k2.dropna(how = 'any', axis = 0) \n",
    "\n",
    "# Shuffling the data and then taking a peek\n",
    "k2 = k2.sample(frac = 1)\n",
    "print(k2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sequence strings into k-mer words, default size = 6 (hexamer words)\n",
    "def getKmers(sequence, size = 6):\n",
    "    return [sequence[x:x + size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "k2['words'] = k2.apply(lambda x: getKmers(x['Sequence']), axis = 1)\n",
    "k2 = k2.drop('Sequence', axis=1)\n",
    "\n",
    "k2_texts = list(k2['words'])\n",
    "for item in range(len(k2_texts)):\n",
    "    k2_texts[item] = ' '.join(k2_texts[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of y is: (999,)\n"
     ]
    }
   ],
   "source": [
    "# Creating y and printing the shape of it\n",
    "y = k2.iloc[:, 0].values\n",
    "print(\"The shape of y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x matrix is: (999, 4079)\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model using CountVectorizer()\n",
    "# This is equivalent to k-mer counting\n",
    "# The n-gram size of 4 was previously determined by testing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(4,4))\n",
    "x = cv.fit_transform(k2_texts)\n",
    "\n",
    "# Print the shape of x\n",
    "print(\"The shape of x matrix is:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train is: (799, 4079)\n",
      "The shape of y_train is: (799,)\n",
      "The shape of x_test is: (200, 4079)\n",
      "The shape of y_test is: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the human dataset into the training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42, stratify=y)\n",
    "\n",
    "# Printing the shapes of the train and test matrices\n",
    "print(\"The shape of x_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of x_test is:\", X_test.shape)\n",
    "print(\"The shape of y_test is:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(str(X_train[0]).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of the two severities in the train data: 1.4813664596273293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff00c4f38d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the balance of the disease severity\n",
    "print(\"The ratio of the two severities in the train data:\", list(y_train).count(0)/list(y_train).count(1))\n",
    "k2[\"Disease\"].value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of the two severities in the test data: 1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff00b94e048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAL+ElEQVR4nO3db4hm9XmH8etbtyZNQqtmh8Xsut0Ftw0mtCQM1iKUkC3E/CHriyBKabdWGAqmTZpCXNsXvgooLU1TaANLtNmCGMWmuCQhjWyVUIomYyJG3RgXE3UXdSdE+y+lySZ3X8xJM4yzmZnnzOzj3l4fWOY5v3POnPvFcHk48zxjqgpJUi8/N+0BJEkbz7hLUkPGXZIaMu6S1JBxl6SGjLskNbRl2gMAbN26tXbt2jXtMSTprPLQQw99t6pmVtr3ioj7rl27mJ+fn/YYknRWSfL06fb5WEaSGjLuktSQcZekhoy7JDVk3CWpoVXjnuS2JCeTPLpk7S+SfDPJI0n+Kcl5S/bdmORYkieSvGuzBpcknd5a7tw/DVyxbO1e4K1V9WvAt4AbAZJcAlwNvGU45++SnLNh00qS1mTVuFfVl4HvLVv7UlWdGjYfAHYMr/cBn6mq/62qbwPHgEs3cF5J0hpsxIeY/gC4c3i9ncXY/8TxYe1lkswBcwA7d+7cgDE2364Dn5/2CK185+b3TnsEqa1Rv1BN8ufAKeD29Z5bVQeraraqZmdmVvz0rCRpQhPfuSf5feB9wN766f+r7wRw0ZLDdgxrkqQzaKI79yRXAB8F3l9V31+y6zBwdZLXJNkN7AG+Mn5MSdJ6rHrnnuQO4B3A1iTHgZtYfHfMa4B7kwA8UFV/WFWPJbkLeJzFxzXXV9WPNmt4SdLKVo17VV2zwvKtP+P4jwEfGzOUJGkcP6EqSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ2tGvcktyU5meTRJWsXJLk3yZPD1/OH9ST5myTHkjyS5O2bObwkaWVruXP/NHDFsrUDwJGq2gMcGbYB3g3sGf7NAZ/cmDElSeuxatyr6svA95Yt7wMODa8PAVcuWf+HWvQAcF6SCzdqWEnS2kz6zH1bVT03vH4e2Da83g48u+S448OaJOkMGv0L1aoqoNZ7XpK5JPNJ5hcWFsaOIUlaYtK4v/CTxy3D15PD+gngoiXH7RjWXqaqDlbVbFXNzszMTDiGJGklk8b9MLB/eL0fuGfJ+u8N75q5DPj3JY9vJElnyJbVDkhyB/AOYGuS48BNwM3AXUmuA54GrhoO/wLwHuAY8H3g2k2YWZK0ilXjXlXXnGbX3hWOLeD6sUNJksbxE6qS1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGRsU9yZ8keSzJo0nuSPLaJLuTPJjkWJI7k5y7UcNKktZm4rgn2Q78MTBbVW8FzgGuBm4BPl5VFwMvAtdtxKCSpLUb+1hmC/ALSbYArwOeA94J3D3sPwRcOfIakqR12jLpiVV1IslfAs8A/wN8CXgIeKmqTg2HHQe2r3R+kjlgDmDnzp2TjiEJ2HXg89MeoZXv3PzeaY8w2pjHMucD+4DdwJuA1wNXrPX8qjpYVbNVNTszMzPpGJKkFYx5LPPbwLeraqGqfgh8FrgcOG94TAOwAzgxckZJ0jqNifszwGVJXpckwF7gceA+4APDMfuBe8aNKElar4njXlUPsviL068B3xi+10HgBuAjSY4BbwRu3YA5JUnrMPEvVAGq6ibgpmXLTwGXjvm+kqRx/ISqJDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDU0Ku5Jzktyd5JvJjma5DeTXJDk3iRPDl/P36hhJUlrM/bO/RPAF6vqzcCvA0eBA8CRqtoDHBm2JUln0MRxT/JLwG8BtwJU1Q+q6iVgH3BoOOwQcOXYISVJ6zPmzn03sAD8fZKvJ/lUktcD26rqueGY54FtY4eUJK3PmLhvAd4OfLKq3gb8N8sewVRVAbXSyUnmkswnmV9YWBgxhiRpuTFxPw4cr6oHh+27WYz9C0kuBBi+nlzp5Ko6WFWzVTU7MzMzYgxJ0nITx72qngeeTfKrw9Je4HHgMLB/WNsP3DNqQknSum0Zef4fAbcnORd4CriWxf9g3JXkOuBp4KqR15AkrdOouFfVw8DsCrv2jvm+kqRx/ISqJDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpodFxT3JOkq8n+dywvTvJg0mOJbkzybnjx5QkrcdG3Ll/CDi6ZPsW4ONVdTHwInDdBlxDkrQOo+KeZAfwXuBTw3aAdwJ3D4ccAq4ccw1J0vqNvXP/a+CjwI+H7TcCL1XVqWH7OLB9pROTzCWZTzK/sLAwcgxJ0lITxz3J+4CTVfXQJOdX1cGqmq2q2ZmZmUnHkCStYMuIcy8H3p/kPcBrgV8EPgGcl2TLcPe+AzgxfkxJ0npMfOdeVTdW1Y6q2gVcDfxLVf0OcB/wgeGw/cA9o6eUJK3LZrzP/QbgI0mOsfgM/tZNuIYk6WcY81jm/1XV/cD9w+ungEs34vtKkibjJ1QlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqaGJ457koiT3JXk8yWNJPjSsX5Dk3iRPDl/P37hxJUlrMebO/RTwp1V1CXAZcH2SS4ADwJGq2gMcGbYlSWfQxHGvqueq6mvD6/8EjgLbgX3AoeGwQ8CVY4eUJK3PhjxzT7ILeBvwILCtqp4bdj0PbNuIa0iS1m503JO8AfhH4MNV9R9L91VVAXWa8+aSzCeZX1hYGDuGJGmJUXFP8vMshv32qvrssPxCkguH/RcCJ1c6t6oOVtVsVc3OzMyMGUOStMyYd8sEuBU4WlV/tWTXYWD/8Ho/cM/k40mSJrFlxLmXA78LfCPJw8PanwE3A3cluQ54Grhq3IiSpPWaOO5V9a9ATrN776TfV5I0np9QlaSGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNbRpcU9yRZInkhxLcmCzriNJerlNiXuSc4C/Bd4NXAJck+SSzbiWJOnlNuvO/VLgWFU9VVU/AD4D7Nuka0mSltmySd93O/Dsku3jwG8sPSDJHDA3bP5Xkic2aZZXo63Ad6c9xGpyy7Qn0BT4s7mxfvl0OzYr7quqqoPAwWldv7Mk81U1O+05pOX82TxzNuuxzAngoiXbO4Y1SdIZsFlx/yqwJ8nuJOcCVwOHN+lakqRlNuWxTFWdSvJB4J+Bc4DbquqxzbiWVuTjLr1S+bN5hqSqpj2DJGmD+QlVSWrIuEtSQ8Zdkhqa2vvctXGSvJnFTwBvH5ZOAIer6uj0ppI0Td65n+WS3MDin3cI8JXhX4A7/INteqVKcu20Z+jOd8uc5ZJ8C3hLVf1w2fq5wGNVtWc6k0mnl+SZqto57Tk687HM2e/HwJuAp5etXzjsk6YiySOn2wVsO5OzvBoZ97Pfh4EjSZ7kp3+sbSdwMfDBqU0lLQb8XcCLy9YD/NuZH+fVxbif5arqi0l+hcU/s7z0F6pfraofTW8yic8Bb6iqh5fvSHL/mR/n1cVn7pLUkO+WkaSGjLskNWTcJakh4y5JDRl3SWro/wCzj3h/voa/NAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The ratio of the two severities in the test data:\", list(y_test).count(0)/list(y_test).count(1))\n",
    "pd.Series(y_test).value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0124 17:26:06.302472 140670309459776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0124 17:26:06.330181 140670309459776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0124 17:26:06.333390 140670309459776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4079, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 17:26:06.424522 140670309459776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0124 17:26:06.447332 140670309459776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0124 17:26:06.451678 140670309459776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 17:26:07.903027 140670309459776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "799/799 [==============================] - 0s 364us/step - loss: 0.6545 - acc: 0.5882\n",
      "Epoch 2/150\n",
      "799/799 [==============================] - 0s 55us/step - loss: 0.6177 - acc: 0.5995\n",
      "Epoch 3/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.6016 - acc: 0.6095\n",
      "Epoch 4/150\n",
      "799/799 [==============================] - 0s 56us/step - loss: 0.6074 - acc: 0.5682\n",
      "Epoch 5/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.5909 - acc: 0.6270\n",
      "Epoch 6/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.5866 - acc: 0.6283\n",
      "Epoch 7/150\n",
      "799/799 [==============================] - 0s 78us/step - loss: 0.5783 - acc: 0.6508\n",
      "Epoch 8/150\n",
      "799/799 [==============================] - 0s 73us/step - loss: 0.5784 - acc: 0.6358\n",
      "Epoch 9/150\n",
      "799/799 [==============================] - 0s 74us/step - loss: 0.5819 - acc: 0.6345\n",
      "Epoch 10/150\n",
      "799/799 [==============================] - 0s 73us/step - loss: 0.5705 - acc: 0.6546\n",
      "Epoch 11/150\n",
      "799/799 [==============================] - 0s 73us/step - loss: 0.5607 - acc: 0.6846\n",
      "Epoch 12/150\n",
      "799/799 [==============================] - 0s 74us/step - loss: 0.5631 - acc: 0.6696\n",
      "Epoch 13/150\n",
      "799/799 [==============================] - 0s 78us/step - loss: 0.5573 - acc: 0.6821\n",
      "Epoch 14/150\n",
      "799/799 [==============================] - 0s 72us/step - loss: 0.5488 - acc: 0.6834\n",
      "Epoch 15/150\n",
      "799/799 [==============================] - 0s 73us/step - loss: 0.5523 - acc: 0.6834\n",
      "Epoch 16/150\n",
      "799/799 [==============================] - 0s 72us/step - loss: 0.5441 - acc: 0.6871\n",
      "Epoch 17/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.5388 - acc: 0.6834\n",
      "Epoch 18/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5434 - acc: 0.6733\n",
      "Epoch 19/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.5234 - acc: 0.7134\n",
      "Epoch 20/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.5349 - acc: 0.6846\n",
      "Epoch 21/150\n",
      "799/799 [==============================] - 0s 64us/step - loss: 0.5189 - acc: 0.7021\n",
      "Epoch 22/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.5170 - acc: 0.7196\n",
      "Epoch 23/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5359 - acc: 0.6959\n",
      "Epoch 24/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5367 - acc: 0.6934\n",
      "Epoch 25/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.5421 - acc: 0.6896\n",
      "Epoch 26/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5170 - acc: 0.6846\n",
      "Epoch 27/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5151 - acc: 0.6859\n",
      "Epoch 28/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.5553 - acc: 0.6834\n",
      "Epoch 29/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5024 - acc: 0.7134\n",
      "Epoch 30/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5042 - acc: 0.7021\n",
      "Epoch 31/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.5012 - acc: 0.7196\n",
      "Epoch 32/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4977 - acc: 0.7209\n",
      "Epoch 33/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4972 - acc: 0.7059\n",
      "Epoch 34/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.5155 - acc: 0.6971\n",
      "Epoch 35/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.5091 - acc: 0.7021\n",
      "Epoch 36/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4963 - acc: 0.7109\n",
      "Epoch 37/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4884 - acc: 0.7222\n",
      "Epoch 38/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4894 - acc: 0.7222\n",
      "Epoch 39/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4838 - acc: 0.7284\n",
      "Epoch 40/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4791 - acc: 0.7447\n",
      "Epoch 41/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4972 - acc: 0.6971\n",
      "Epoch 42/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4926 - acc: 0.6959\n",
      "Epoch 43/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4856 - acc: 0.7247\n",
      "Epoch 44/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.5257 - acc: 0.6971\n",
      "Epoch 45/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.5135 - acc: 0.6984\n",
      "Epoch 46/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4895 - acc: 0.7046\n",
      "Epoch 47/150\n",
      "799/799 [==============================] - 0s 67us/step - loss: 0.4789 - acc: 0.7184\n",
      "Epoch 48/150\n",
      "799/799 [==============================] - 0s 65us/step - loss: 0.4793 - acc: 0.7272\n",
      "Epoch 49/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4769 - acc: 0.7247\n",
      "Epoch 50/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4740 - acc: 0.7284\n",
      "Epoch 51/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4815 - acc: 0.7096\n",
      "Epoch 52/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4901 - acc: 0.6996\n",
      "Epoch 53/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4770 - acc: 0.7059\n",
      "Epoch 54/150\n",
      "799/799 [==============================] - 0s 67us/step - loss: 0.4725 - acc: 0.7247\n",
      "Epoch 55/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4776 - acc: 0.7196\n",
      "Epoch 56/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.5071 - acc: 0.6959\n",
      "Epoch 57/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4783 - acc: 0.7146\n",
      "Epoch 58/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4713 - acc: 0.7146\n",
      "Epoch 59/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4727 - acc: 0.7159\n",
      "Epoch 60/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4896 - acc: 0.6959\n",
      "Epoch 61/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.4717 - acc: 0.7222\n",
      "Epoch 62/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4830 - acc: 0.7247\n",
      "Epoch 63/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4881 - acc: 0.7096\n",
      "Epoch 64/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4709 - acc: 0.7184\n",
      "Epoch 65/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4695 - acc: 0.7222\n",
      "Epoch 66/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4746 - acc: 0.7071\n",
      "Epoch 67/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4731 - acc: 0.7222\n",
      "Epoch 68/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4681 - acc: 0.7272\n",
      "Epoch 69/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4817 - acc: 0.7109\n",
      "Epoch 70/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4826 - acc: 0.7146\n",
      "Epoch 71/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4838 - acc: 0.6934\n",
      "Epoch 72/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4659 - acc: 0.7297\n",
      "Epoch 73/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4630 - acc: 0.7234\n",
      "Epoch 74/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4576 - acc: 0.7397\n",
      "Epoch 75/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4706 - acc: 0.7096\n",
      "Epoch 76/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4762 - acc: 0.7046\n",
      "Epoch 77/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4682 - acc: 0.7209\n",
      "Epoch 78/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4702 - acc: 0.7297\n",
      "Epoch 79/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4692 - acc: 0.7234\n",
      "Epoch 80/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4631 - acc: 0.7322\n",
      "Epoch 81/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4619 - acc: 0.7159\n",
      "Epoch 82/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4602 - acc: 0.7334\n",
      "Epoch 83/150\n",
      "799/799 [==============================] - 0s 84us/step - loss: 0.4710 - acc: 0.7134\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799/799 [==============================] - 0s 66us/step - loss: 0.4652 - acc: 0.7196\n",
      "Epoch 85/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4994 - acc: 0.7096\n",
      "Epoch 86/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4914 - acc: 0.7059\n",
      "Epoch 87/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4666 - acc: 0.7159\n",
      "Epoch 88/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4628 - acc: 0.7109\n",
      "Epoch 89/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4600 - acc: 0.7234\n",
      "Epoch 90/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4694 - acc: 0.7046\n",
      "Epoch 91/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4678 - acc: 0.7297\n",
      "Epoch 92/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4814 - acc: 0.7021\n",
      "Epoch 93/150\n",
      "799/799 [==============================] - 0s 63us/step - loss: 0.4647 - acc: 0.7272\n",
      "Epoch 94/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4574 - acc: 0.7259\n",
      "Epoch 95/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.4746 - acc: 0.7247\n",
      "Epoch 96/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.4845 - acc: 0.6946\n",
      "Epoch 97/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4581 - acc: 0.7184\n",
      "Epoch 98/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4625 - acc: 0.7196\n",
      "Epoch 99/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4681 - acc: 0.7134\n",
      "Epoch 100/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4745 - acc: 0.7059\n",
      "Epoch 101/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4611 - acc: 0.7196\n",
      "Epoch 102/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4627 - acc: 0.7121\n",
      "Epoch 103/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.4905 - acc: 0.7184\n",
      "Epoch 104/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4702 - acc: 0.7259\n",
      "Epoch 105/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4605 - acc: 0.7222\n",
      "Epoch 106/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4531 - acc: 0.7334\n",
      "Epoch 107/150\n",
      "799/799 [==============================] - 0s 61us/step - loss: 0.4715 - acc: 0.7171\n",
      "Epoch 108/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.5368 - acc: 0.6859\n",
      "Epoch 109/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4791 - acc: 0.6959\n",
      "Epoch 110/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4567 - acc: 0.7372\n",
      "Epoch 111/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4674 - acc: 0.7196\n",
      "Epoch 112/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4707 - acc: 0.7059\n",
      "Epoch 113/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4530 - acc: 0.7384\n",
      "Epoch 114/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4561 - acc: 0.7209\n",
      "Epoch 115/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4558 - acc: 0.7259\n",
      "Epoch 116/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4519 - acc: 0.7459\n",
      "Epoch 117/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4590 - acc: 0.7222\n",
      "Epoch 118/150\n",
      "799/799 [==============================] - 0s 56us/step - loss: 0.4592 - acc: 0.7046\n",
      "Epoch 119/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4599 - acc: 0.7272\n",
      "Epoch 120/150\n",
      "799/799 [==============================] - 0s 56us/step - loss: 0.4659 - acc: 0.7084\n",
      "Epoch 121/150\n",
      "799/799 [==============================] - 0s 56us/step - loss: 0.4647 - acc: 0.7196\n",
      "Epoch 122/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4496 - acc: 0.7409\n",
      "Epoch 123/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4598 - acc: 0.7284\n",
      "Epoch 124/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4569 - acc: 0.7322\n",
      "Epoch 125/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4505 - acc: 0.7159\n",
      "Epoch 126/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4510 - acc: 0.7359\n",
      "Epoch 127/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4598 - acc: 0.7309\n",
      "Epoch 128/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4649 - acc: 0.7209\n",
      "Epoch 129/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4620 - acc: 0.7184\n",
      "Epoch 130/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4713 - acc: 0.7171\n",
      "Epoch 131/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4547 - acc: 0.7259\n",
      "Epoch 132/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4653 - acc: 0.7146\n",
      "Epoch 133/150\n",
      "799/799 [==============================] - 0s 62us/step - loss: 0.4816 - acc: 0.6996\n",
      "Epoch 134/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4559 - acc: 0.7497\n",
      "Epoch 135/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4565 - acc: 0.7372\n",
      "Epoch 136/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4585 - acc: 0.7084\n",
      "Epoch 137/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4526 - acc: 0.7259\n",
      "Epoch 138/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4645 - acc: 0.7247\n",
      "Epoch 139/150\n",
      "799/799 [==============================] - 0s 58us/step - loss: 0.4700 - acc: 0.7372\n",
      "Epoch 140/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4633 - acc: 0.7347\n",
      "Epoch 141/150\n",
      "799/799 [==============================] - 0s 57us/step - loss: 0.4528 - acc: 0.7222\n",
      "Epoch 142/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4757 - acc: 0.6971\n",
      "Epoch 143/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4565 - acc: 0.7121\n",
      "Epoch 144/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4750 - acc: 0.7184\n",
      "Epoch 145/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4993 - acc: 0.7009\n",
      "Epoch 146/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4657 - acc: 0.7146\n",
      "Epoch 147/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4575 - acc: 0.7046\n",
      "Epoch 148/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4545 - acc: 0.7322\n",
      "Epoch 149/150\n",
      "799/799 [==============================] - 0s 60us/step - loss: 0.4515 - acc: 0.7322\n",
      "Epoch 150/150\n",
      "799/799 [==============================] - 0s 59us/step - loss: 0.4475 - acc: 0.7272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff00b256e48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs = 150, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/999 [==============================] - 0s 52us/step\n",
      "Accuracy: 70.67\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(x, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1243)\t1\n",
      "  (0, 3392)\t1\n",
      "  (0, 1016)\t1\n",
      "  (0, 186)\t1\n",
      "  (0, 2712)\t1\n",
      "  (0, 1049)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 3433)\t1\n",
      "  (0, 1968)\t1\n",
      "  (0, 1042)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 1860)\t1\n",
      "  (0, 3904)\t1\n",
      "  (0, 1806)\t1\n",
      "  (0, 3024)\t1\n",
      "  (0, 2740)\t1\n",
      "  (0, 2057)\t1\n",
      "  (0, 2459)\t1\n",
      "  (0, 4038)\t1\n",
      "  (0, 1810)\t1\n",
      "  (0, 3051)\t1\n",
      "  (0, 3176)\t1\n",
      "  (0, 1100)\t1\n",
      "  (0, 1513)\t1\n",
      "  (0, 2209)\t1\n",
      "  :\t:\n",
      "  (0, 1622)\t1\n",
      "  (0, 487)\t1\n",
      "  (0, 3649)\t1\n",
      "  (0, 893)\t1\n",
      "  (0, 975)\t1\n",
      "  (0, 3479)\t1\n",
      "  (0, 364)\t1\n",
      "  (0, 770)\t1\n",
      "  (0, 2901)\t1\n",
      "  (0, 994)\t1\n",
      "  (0, 3867)\t1\n",
      "  (0, 429)\t1\n",
      "  (0, 2160)\t1\n",
      "  (0, 3753)\t1\n",
      "  (0, 2728)\t1\n",
      "  (0, 1635)\t1\n",
      "  (0, 2132)\t1\n",
      "  (0, 3357)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 2833)\t1\n",
      "  (0, 237)\t1\n",
      "  (0, 3546)\t1\n",
      "  (0, 3849)\t1\n",
      "  (0, 220)\t1\n",
      "  (0, 3242)\t1 => 1 (expected 0)\n",
      "  (0, 1243)\t1\n",
      "  (0, 3392)\t1\n",
      "  (0, 1016)\t1\n",
      "  (0, 186)\t1\n",
      "  (0, 2712)\t1\n",
      "  (0, 1049)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 3433)\t1\n",
      "  (0, 1968)\t1\n",
      "  (0, 1042)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 1860)\t1\n",
      "  (0, 3904)\t1\n",
      "  (0, 1806)\t1\n",
      "  (0, 3024)\t1\n",
      "  (0, 2740)\t1\n",
      "  (0, 2057)\t1\n",
      "  (0, 2459)\t1\n",
      "  (0, 2745)\t1\n",
      "  (0, 2121)\t1\n",
      "  (0, 3212)\t1\n",
      "  (0, 1816)\t1\n",
      "  (0, 3148)\t1\n",
      "  (0, 607)\t1\n",
      "  (0, 951)\t1\n",
      "  :\t:\n",
      "  (0, 2237)\t1\n",
      "  (0, 1785)\t1\n",
      "  (0, 2883)\t1\n",
      "  (0, 919)\t1\n",
      "  (0, 1735)\t1\n",
      "  (0, 2360)\t1\n",
      "  (0, 796)\t1\n",
      "  (0, 687)\t1\n",
      "  (0, 1949)\t1\n",
      "  (0, 837)\t1\n",
      "  (0, 3475)\t1\n",
      "  (0, 2611)\t1\n",
      "  (0, 2409)\t1\n",
      "  (0, 1886)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 1213)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 1523)\t1\n",
      "  (0, 2462)\t1\n",
      "  (0, 2812)\t1\n",
      "  (0, 3776)\t1\n",
      "  (0, 3042)\t1\n",
      "  (0, 2980)\t1\n",
      "  (0, 2038)\t1\n",
      "  (0, 1982)\t1 => 1 (expected 1)\n",
      "  (0, 1243)\t1\n",
      "  (0, 3392)\t1\n",
      "  (0, 1016)\t1\n",
      "  (0, 186)\t1\n",
      "  (0, 2712)\t1\n",
      "  (0, 1049)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 3433)\t1\n",
      "  (0, 1968)\t1\n",
      "  (0, 1042)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 1860)\t1\n",
      "  (0, 3904)\t1\n",
      "  (0, 1806)\t1\n",
      "  (0, 3024)\t1\n",
      "  (0, 2740)\t1\n",
      "  (0, 2057)\t1\n",
      "  (0, 2459)\t1\n",
      "  (0, 2745)\t1\n",
      "  (0, 2121)\t1\n",
      "  (0, 3212)\t1\n",
      "  (0, 1816)\t1\n",
      "  (0, 3148)\t1\n",
      "  (0, 607)\t1\n",
      "  (0, 951)\t1\n",
      "  :\t:\n",
      "  (0, 416)\t1\n",
      "  (0, 2083)\t1\n",
      "  (0, 2764)\t1\n",
      "  (0, 2333)\t1\n",
      "  (0, 547)\t1\n",
      "  (0, 207)\t1\n",
      "  (0, 2951)\t1\n",
      "  (0, 3079)\t1\n",
      "  (0, 3703)\t1\n",
      "  (0, 1917)\t1\n",
      "  (0, 510)\t1\n",
      "  (0, 3912)\t1\n",
      "  (0, 1938)\t1\n",
      "  (0, 679)\t1\n",
      "  (0, 1868)\t1\n",
      "  (0, 4013)\t1\n",
      "  (0, 317)\t1\n",
      "  (0, 3132)\t1\n",
      "  (0, 266)\t1\n",
      "  (0, 3744)\t1\n",
      "  (0, 2673)\t1\n",
      "  (0, 3606)\t1\n",
      "  (0, 503)\t1\n",
      "  (0, 3902)\t1\n",
      "  (0, 1541)\t1 => 0 (expected 1)\n",
      "  (0, 1243)\t1\n",
      "  (0, 3392)\t1\n",
      "  (0, 1016)\t1\n",
      "  (0, 186)\t1\n",
      "  (0, 2712)\t1\n",
      "  (0, 1049)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 3433)\t1\n",
      "  (0, 1968)\t1\n",
      "  (0, 1042)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 1860)\t1\n",
      "  (0, 3904)\t1\n",
      "  (0, 1806)\t1\n",
      "  (0, 3024)\t1\n",
      "  (0, 2740)\t1\n",
      "  (0, 2057)\t1\n",
      "  (0, 2459)\t1\n",
      "  (0, 4038)\t1\n",
      "  (0, 1810)\t1\n",
      "  (0, 3051)\t1\n",
      "  (0, 3176)\t1\n",
      "  (0, 1100)\t1\n",
      "  (0, 1513)\t1\n",
      "  (0, 2209)\t1\n",
      "  :\t:\n",
      "  (0, 1622)\t1\n",
      "  (0, 487)\t1\n",
      "  (0, 3649)\t1\n",
      "  (0, 893)\t1\n",
      "  (0, 975)\t1\n",
      "  (0, 3479)\t1\n",
      "  (0, 364)\t1\n",
      "  (0, 770)\t1\n",
      "  (0, 2901)\t1\n",
      "  (0, 994)\t1\n",
      "  (0, 3867)\t1\n",
      "  (0, 429)\t1\n",
      "  (0, 2160)\t1\n",
      "  (0, 3753)\t1\n",
      "  (0, 2728)\t1\n",
      "  (0, 1635)\t1\n",
      "  (0, 2132)\t1\n",
      "  (0, 3357)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 2833)\t1\n",
      "  (0, 237)\t1\n",
      "  (0, 3546)\t1\n",
      "  (0, 3849)\t1\n",
      "  (0, 220)\t1\n",
      "  (0, 3242)\t1 => 1 (expected 1)\n",
      "  (0, 1243)\t1\n",
      "  (0, 3392)\t1\n",
      "  (0, 1016)\t1\n",
      "  (0, 186)\t1\n",
      "  (0, 2712)\t1\n",
      "  (0, 1049)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 3433)\t1\n",
      "  (0, 1968)\t1\n",
      "  (0, 1042)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 1860)\t1\n",
      "  (0, 3904)\t1\n",
      "  (0, 1806)\t1\n",
      "  (0, 3024)\t1\n",
      "  (0, 2740)\t1\n",
      "  (0, 2057)\t1\n",
      "  (0, 2459)\t1\n",
      "  (0, 2745)\t1\n",
      "  (0, 2121)\t1\n",
      "  (0, 3212)\t1\n",
      "  (0, 1816)\t1\n",
      "  (0, 3148)\t1\n",
      "  (0, 607)\t1\n",
      "  (0, 951)\t1\n",
      "  :\t:\n",
      "  (0, 1913)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 2491)\t1\n",
      "  (0, 3568)\t1\n",
      "  (0, 4036)\t1\n",
      "  (0, 1808)\t1\n",
      "  (0, 3049)\t1\n",
      "  (0, 3173)\t1\n",
      "  (0, 1096)\t1\n",
      "  (0, 1483)\t1\n",
      "  (0, 1446)\t1\n",
      "  (0, 365)\t1\n",
      "  (0, 771)\t1\n",
      "  (0, 2902)\t1\n",
      "  (0, 995)\t1\n",
      "  (0, 3868)\t1\n",
      "  (0, 430)\t1\n",
      "  (0, 2165)\t1\n",
      "  (0, 3774)\t1\n",
      "  (0, 3036)\t1\n",
      "  (0, 670)\t1\n",
      "  (0, 1673)\t1\n",
      "  (0, 1348)\t1\n",
      "  (0, 1240)\t1\n",
      "  (0, 3381)\t1 => 0 (expected 0)\n"
     ]
    }
   ],
   "source": [
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X_test)\n",
    "# summarize the first 5 cases\n",
    "for i in range(5):\n",
    "    print('%s => %d (expected %d)' % (X_test[i], predictions[i], y_test[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
